# Логарифмическая регрессия в нашей модели

## 1) Что такое регрессия
**Регрессия** — это статистический метод, который по данным наблюдений оценивает коэффициенты выбранного уравнения связи между переменными.

В нашем исследовании:
- X (предикторы): параметры нагрузки — объём V (кг), количество подъёмов P (раз), отдых R (мин).
- Y (отклик): концентрация биомаркера в моче (креатинин, белок, миоглобин).

Регрессия отвечает на вопрос: *какие коэффициенты лучше всего описывают наши данные при выбранной форме уравнения*.

## 2) Чем регрессия отличается от функции
**Функция** в математике — это точная зависимость: каждому X соответствует строго одно Y.

**Регрессия** — это оценка зависимости по шумным данным:
- На практике при одинаковых V, P, R маркер может отличаться из‑за сна, питания, гидратации, стресса, температуры, погрешности лаборатории.
- Поэтому регрессионная модель описывает **средний ожидаемый отклик** и неизбежно имеет **ошибку прогноза**.

Иными словами: функция — «идеальный закон», регрессия — «лучшее приближение закона по данным».

## 3) Почему множественная линейная
У нас не один фактор, а три управляемых параметра. Поэтому модель должна учитывать вклад каждого:

- объём V;
- количество подъёмов P (в вашей трактовке P обратно связано с «тяжестью»: меньше подъёмов → выше средний вес);
- отдых R.

Форма уравнения берётся линейной по параметрам, потому что:
- она хорошо интерпретируется;
- её можно устойчиво оценить на небольших выборках;
- коэффициенты сразу становятся «рычагами управления».

## 4) Почему логарифмическая
Мы логарифмируем **маркер**, то есть строим модель для ln(Y), а не для самого Y:

```
ln(Y) = b0 + bV*ΔV + bP*ΔP + bR*ΔR
```

Где ΔV, ΔP, ΔR — отклонения от базового режима спортсмена (например, от среднего за период наблюдения).

Зачем это нужно:
1) **Процентная интерпретация.** В лог‑модели влияние факторов становится мультипликативным (в процентах), что физиологически правдоподобно.
2) **Стабилизация разброса.** Концентрации маркеров часто имеют правостороннюю асимметрию и рост вариативности при росте уровня; логарифм это сглаживает.

Конкретный пример: миоглобин вырос с 30 до 60 мкг/л.
- Абсолютно: +30 мкг/л.
- Относительно: ×2 (то есть +100%).

В лог‑шкале «×2» даёт одно и то же приращение независимо от исходного уровня:
- ln(60) − ln(30) = ln(2) ≈ 0.693.

## 5) Как читать коэффициенты
Если у нас:

```
ln(M) = b0 + bV*ΔV + bP*ΔP + bR*ΔR
```

то при изменении параметра на 1 единицу маркер изменяется в exp(b) раз.

Пример (миоглобин): если bR = −0.38, то +1 мин отдыха даёт множитель exp(−0.38) ≈ 0.684, то есть примерно −31.6% к миоглобину.

Важно: для V обычно удобно интерпретировать шагами по 1000 кг (иначе эффект на 1 кг слишком мал).

## 5.1) Как мы получили каждый коэффициент (b0, bV, bP, bR)
Коэффициенты регрессии **не “подбираются вручную”**: они вычисляются по данным наблюдений методом наименьших квадратов (МНК).
Для **каждого маркера** (креатинин / белок / миоглобин) строится **своя отдельная** регрессия, поэтому набор коэффициентов \(b\) — индивидуальный.

### Шаг 1. Подготовка данных (что такое \(X\) и \(y\))
Для каждого наблюдения \(i\) (тренировка/замер) мы имеем:
- \(V_i, P_i, R_i\) — параметры нагрузки,
- \(Y_i\) — концентрация маркера.

Далее:
- делаем отклики \(y_i = \ln(Y_i)\),
- задаём “базу” спортсмена \((V_0, P_0, R_0)\) (обычно средние за период наблюдения),
- считаем отклонения \(\Delta V_i, \Delta P_i, \Delta R_i\).

Важно: \(\Delta\) можно задавать по-разному, и от этого зависит “масштаб” коэффициента:
- **абсолютно**: \(\Delta V_i = V_i - V_0\) (в кг), \(\Delta P_i = P_i - P_0\) (в разах), \(\Delta R_i = R_i - R_0\) (в мин);
- **относительно (проценты)**: \(\Delta V_i = (V_i - V_0)/V_0\) и т.д. (удобно для интерпретации).

### Шаг 2. Строим регрессионную “таблицу признаков” (дизайн‑матрицу)
Для каждого наблюдения формируем строку:

```
X_i = [ 1,  ΔV_i,  ΔP_i,  ΔR_i ]
```

А вектор коэффициентов:

```
b = [ b0, bV, bP, bR ]ᵀ
```

И модель записывается компактно:

```
y_i = X_i · b + ε_i
```

### Шаг 3. Оценка коэффициентов (МНК)
МНК выбирает такие коэффициенты, чтобы суммарная квадратичная ошибка была минимальной:

\[
\hat b = \arg\min_b \sum_{i=1}^{N} (y_i - X_i b)^2
\]

Решение (закрытая формула):

```
b̂ = (XᵀX)⁻¹ Xᵀ y
```

Где \(y\) — столбец всех \(y_i = \ln(Y_i)\), а \(X\) — матрица всех строк \(X_i\).

### Что означает “каждый коэффициент” и откуда он берётся
- **\(b0\) (свободный член)**: это прогноз \(\ln(Y)\) при “базовых” значениях, то есть когда \(\Delta V=\Delta P=\Delta R=0\).
  - В исходных единицах это \(Y_0 = \exp(b0)\).
- **\(bV\)**: показывает, на сколько меняется \(\ln(Y)\) при увеличении **\(\Delta V\) на 1** при фиксированных \(\Delta P\) и \(\Delta R\).
  - То есть это **частный эффект V** (“вклад V при прочих равных”), который МНК оценивает по тому, как менялся маркер, когда V менялся *не в точности вместе* с P и R.
- **\(bP\)**: аналогично — частный эффект P (при фиксированных V и R).
- **\(bR\)**: частный эффект R (при фиксированных V и P).

Почему это важно: при множественной регрессии коэффициент — это не “простая корреляция”, а эффект **после учёта** остальных факторов.

### Перевод коэффициентов в проценты (короткая памятка)
Если \(\Delta\) относительные (например +0.10 = +10%), то для изменения, например, объёма на \(+10\%\):

```
Δln(Y) = bV * 0.10
Y / Y0 = exp(Δln(Y))
```

Если \(\Delta\) абсолютные, то в проценты обычно переводят через “шаг” (например, \(ΔV=+1000\) кг).

---

## 6) 4 постулата: как они **вытекают из регрессии** (с примерами)

Ниже показано, что «4 постулата управления нагрузкой» — это не отдельная философия, а прямые следствия одной и той же формулы:

```
ln(Y) = b0 + b1·ΔV + b2·ΔP + b3·ΔR
```

Где:
- ΔV, ΔP, ΔR — отклонения от базового режима (например, относительные: ΔV=(V−V0)/V0 и т.д.)
- Y — конкретный маркер (креатинин / белок / миоглобин); для каждого маркера будут свои b.

Чтобы примеры были «живые», возьмём один маркер (например, **белок**) и условные коэффициенты (они всегда индивидуальные):

- b1 (для ΔV) = **+0.60**
- b2 (для ΔP) = **+0.20**
- b3 (для ΔR) = **−0.50**

База спортсмена:
- V0 = 8000 кг/тренировку (условно)
- P0 = 60 подъёмов
- R0 = 2.0 мин
- Y0 = 2.0 (условные единицы), тогда ln(Y0)=ln(2.0)

### Постулат 1. Индивидуальная чувствительность

**Факт из регрессии:** чувствительность к параметру определяется величиной соответствующего коэффициента.

В лог‑модели вклад каждого параметра в ln(Y) аддитивен:

```
вклад V = b1·ΔV
вклад P = b2·ΔP
вклад R = b3·ΔR
```

Поэтому, если сравнивать |b|, мы получаем «что сильнее рычаг»:

- |b1|=0.60 (V) — сильный эффект
- |b3|=0.50 (R) — почти такой же сильный
- |b2|=0.20 (P) — слабее

**Вывод:** этот спортсмен по белку наиболее чувствителен к V и R (а P — менее критичный рычаг).
Это и есть формализованная «индивидуальность» в терминах модели.

### Постулат 2. Скорость изменения маркеров (процентный эффект)

**Факт из лог‑регрессии:** ln(Y) — это логарифм, а значит изменение ln(Y) соответствует *процентному/мультипликативному* изменению Y.

Если меняем только один параметр, например увеличили объём на 10%:

- ΔV = +0.10
- ΔP = 0
- ΔR = 0

Тогда:

```
Δln(Y) = b1·ΔV = 0.60·0.10 = 0.06
Y / Y0 = exp(Δln(Y)) = exp(0.06) ≈ 1.062
```

**Интерпретация:** белок вырастет примерно на **+6.2%**.

То есть коэффициент b не «в единицах маркера», а в единицах *скорости относительного изменения* — именно поэтому модель удобна для управления.

### Постулат 3. Приоритет коррекции (какой параметр менять, если маркер вышел из зоны)

**Факт из регрессии:** если нам нужно быстро “вернуть” маркер в целевой коридор, рационально менять параметр,
который даёт максимальный эффект на ln(Y) при минимальной практической цене изменения.

Пример: после тренировки белок оказался выше целевого на 20% относительно базы.

В лог‑шкале это:

```
нужно уменьшить ln(Y) на ln(1.20) ≈ 0.182
```

Самый простой рычаг обычно **R** (изменить паузы проще, чем перестраивать весь объём/структуру).
Посмотрим, на сколько нужно изменить R, если трогаем только R:

```
Δln(Y) = b3·ΔR  ⇒  ΔR = (-0.182) / (-0.50) = +0.364
```

Если ΔR считается относительным, то:

```
R = R0·(1+ΔR) = 2.0·(1+0.364) ≈ 2.73 мин
```

**Вывод:** чтобы “снять” ~20% по белку, достаточно увеличить паузы примерно с 2.0 до **2.7 мин**,
не ломая полностью тренировку по V и P.

Это и есть логика приоритета: большой |b| + низкая стоимость изменения = оптимальный рычаг.

### Постулат 4. Компенсация параметров (чтобы план был согласованным)

**Факт из линейной формы по ln(Y):** разные параметры могут компенсировать друг друга, потому что их вклады суммируются.

Если мы хотим, чтобы маркер **в среднем не изменился** относительно базы (то есть Δln(Y)≈0), то из модели:

```
b1·ΔV + b2·ΔP + b3·ΔR ≈ 0
```

Отсюда сразу следует формула компенсации через R:

```
ΔR ≈ -(b1·ΔV + b2·ΔP) / b3
```

Пример: мы хотим сделать тренировку «тяжелее» по смыслу (P↓), но при этом немного поднять объём:

- ΔV = +0.15 (объём +15%)
- ΔP = −0.10 (подъёмы −10% → средний вес выше)

Тогда:

```
ΔR ≈ -(0.60·0.15 + 0.20·(-0.10)) / (-0.50)
   = -(0.09 - 0.02) / (-0.50)
   = -0.07 / (-0.50)
   = +0.14
```

То есть:

```
R = 2.0·(1+0.14) = 2.28 мин
```

**Интерпретация:** увеличение пауз примерно до **2.3 мин** компенсирует одновременный рост V и снижение P так,
что прогноз по белку остаётся примерно на базовом уровне.

Это ключ к «связности графиков» и корректному плану: V и P задают тренировочный стимул, а R используется как компенсатор,
который удерживает маркеры в допустимом коридоре.

---

### Итоговая связка “постулаты ⇄ регрессия”

- Постулат 1 (чувствительность) — это сравнение |b1|,|b2|,|b3|.
- Постулат 2 (скорость в %) — это exp(Δln(Y)) и интерпретация коэффициентов в относительных изменениях.
- Постулат 3 (приоритет коррекции) — это выбор параметра с большим |b| и минимальной стоимостью изменения (часто R).
- Постулат 4 (компенсация) — это прямое следствие линейности ln(Y): b1·ΔV + b2·ΔP + b3·ΔR ≈ 0 и решение ΔR.

---

## 7) Обоснование минимального объёма данных и степеней свободы

В исследовании используется множественная лог-линейная регрессионная модель вида:

```
ln(Y) = b0 + b1·ΔV + b2·ΔP + b3·ΔR + ε
```

где оцениваются **четыре параметра** модели (k = 4): свободный член b0 и коэффициенты при трёх тренировочных факторах.

### Оценка коэффициентов (МНК) и идентифицируемость

Оценка параметров осуществляется методом наименьших квадратов:

```
b̂ = (XᵀX)⁻¹ Xᵀ y
```

Это требует выполнения условия идентифицируемости (возможности оценить параметры):

```
N ≥ k
```

где:
- **N** — число наблюдений (тренировок/замеров),
- **k** — число оцениваемых параметров модели.

Если N < k, матрица (XᵀX) становится вырожденной (или плохо обусловленной) и устойчиво оценить коэффициенты невозможно.

### Степени свободы для остаточной дисперсии

После оценки параметров число степеней свободы для оценки остаточной дисперсии:

```
df = N − k
```

Остаточная дисперсия оценивается так:

```
σ̂² = ( Σᵢ₌₁ᴺ ε̂ᵢ² ) / (N − k)
```

и используется для расчёта стандартных ошибок коэффициентов и доверительных интервалов.

### Почему важны “не слишком малые” df

При малом числе степеней свободы (df близком к нулю):

- оценка σ̂² становится **нестабильной** (сильно меняется при добавлении/удалении одной точки),
- стандартные ошибки коэффициентов становятся **ненадёжными**,
- сами коэффициенты регрессии становятся **чувствительными к отдельным наблюдениям** (высокий риск переобучения на шум).

В прикладных задачах регрессионного анализа обычно считают разумным иметь **не менее ~8 степеней свободы** для более устойчивой интерпретации результатов на уровне индивидуальной модели.

### Минимальный “рабочий” объём данных

С учётом числа оцениваемых параметров:

```
N = k + df = 4 + 8 = 12
```

Таким образом, при **N = 12** наблюдениях модель:

- остаётся идентифицируемой (N ≥ k),
- имеет достаточно степеней свободы для оценки остаточной дисперсии,
- позволяет вычислять стандартные ошибки коэффициентов,
- может использоваться как **пилотная индивидуальная модель** прогнозирования и управления тренировочной нагрузкой.

При меньшем числе наблюдений (N < 12) модель остаётся математически определимой (при N ≥ k),
но характеризуется повышенной нестабильностью оценок и сниженной достоверностью количественных выводов —
поэтому количество измерений необходимо обосновывать через k и df.
